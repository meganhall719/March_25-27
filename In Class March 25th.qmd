---
title: "In Class March 25th"
format: html
editor: visual
---

## Elements of Regression

### Goal

to partition variance in the outcome/response variable among different sources, i.e., into that explained by the regression model itself versus the left-over error or residual variance

SSR \<- portion explained by our model

SSE \<- combined error term of each of our predicted estimates from our observed values of y

SSY = SSR + SSE

regression is anaylysis of how varaince is partitioned

```{r}
library(tidyverse)
library(ggplot2)
f <- "https://raw.githubusercontent.com/difiore/ada-datasets/refs/heads/main/zombies.csv"

d <- read_csv(f, col_names = TRUE)
head(d)
```

```{r}
#Regression model with our zombie apocalupse survivor dataset 

m <- lm(data = d, height ~ weight)
summary (m)


SSY <- sum((m$model$height - mean (m$model$height))^2)

SSY

#SSR = predicted height - mean height
SSR <- sum((m$fitted.values - mean(m$model$height))^2)
SSR

#SSE = height - predicted height
SSE <- sum((m$model$height - m$fitted.values) ^ 2)
SSE

#mean overall variance 
MSY <- SSY/(nrow(d)-1)
```

Calculating the variance in each of the components also called mean square by dividing SSY, SSR, and SSE by its corresponding \# of degrees of freedom

## Anova tables : Mean Square

```{r}

#mean overall variance 
MSY <- SSY/(nrow(d)-1)

#mean overall varaince 

MSR <- SSR / (1)

#mean reamining variance
MSE <- SSE/(nrow(d) -1 -1 )

#Fration ratio of explained to unexplained variance
#high fratio regression explains it 

fratio <-MSR/MSE

#frratio is our test statistic 

#p value - proportion of f distribution that lies between 0 and fratio value 

pf( q = fratio, df1 = 1, df2 = 998, 
    lower.tail = FALSE)
```

## Fratio

asking r to return the upper tail , to the right of the f-ratio

```{r}
#Fration ratio of explained to unexplained variance
#high fratio regression explains it 

fratio <-MSR/MSE

#frratio is our test statistic 

#p value - proportion of f distribution that lies between 0 and fratio value 

pf( q = fratio, df1 = 1, df2 = 998, 
    lower.tail = FALSE)

# the fratio is 2.646279e-258 very small 

# or 
1-pf(q = fratio, df1 = 1, df2 = 998)


```

### draw fratio

the proportion of the variance explained is high

it is very unlikely that we would see an fratio that high by chance you can reject your null hypothesis

```{r}
mosaic::plotDist("f", df1 = 1, df2 = 998)
```

```{r}
(rsq <-SSR/SSY)
summary(m) # looks at multiple R - squared value 
# r calculates this mmean squared some of squares, you can do it by hand but r does it to 

# 0.6931998 ~ 69.3 % our model does a really good job at explaining variation in y variable 

anova(m) # get out analysis of variance table 

```

## uncorrelated random data

no relation best estimate of x will be an average of y

```{r}
new_d <-tibble (x = rnorm (1000, mean = 100, sd = 25),
                y = rnorm(1000, mean = 10, sd = 2))

plot(new_d$x, new_d$y)
m <- lm(y ~x, data = new_d)

summary(m)
```

```{r}

#SSY
SSY <- sum((m$model$y - mean(m$model$y)) ^ 2 )

#SSR
SSR <- sum((m$fitted.values - mean(m$model$y)) ^2)

#SSE

SSE <- sum((m$model$y - m$fitted.values) ^ 2 )

#mean overall variance 
MSY <- SSY/(nrow(d)-1)

#mean overall varaince 

MSR <- SSR / (1)

#mean reamining variance
MSE <- SSE/(nrow(d) -1 -1 )

fratio <- MSR/ MSE
fratio
# 2.17473

anova(m)

```

## Model Checking : Four conditions for inference

### L

linearlity of relationshiib between varialbes

### i

indepencencs of residulats

### N

normality of residuals

look at histogram of residuals should be normally distributed and centered around 0

### E

equality of variance (homoscedasticity ) of the resitduals

across range of x what is the variance of x

#### L, N, E

can be evaluated by residual anaylysis

### I

depends on how data were collected

## how to check of LN

we can plot our ersponse variable versus our predcictor

we can plot a histogram of residulas

we can plot residulats in relation of predictor

use plot function

```{r}
library(tidyverse)
library(ggplot2)
f <- "https://raw.githubusercontent.com/difiore/ada-datasets/refs/heads/main/zombies.csv"

d <- read_csv(f, col_names = TRUE)
head(d)

m <- lm(data = d, height ~ weight)
summary (m)

m <- lm (data = d, height ~weight)

par(mfrow = c(2,2))

plot(m)


```

### Coding Challenge - Exercise 9

```{r}

library(tidyverse)
 
f <- "https://raw.githubusercontent.com/difiore/ada-datasets/refs/heads/main/KamilarAndCooperData.csv"
d <- read_csv(f, col_names = TRUE)

#plot lifespan veruss female body mass
lifespan_model <- lm(MaxLongevity_m ~ Body_mass_female_mean, data = d)

plot(lifespan_model)

#In the linear regression model lm(MaxLongevity_m ~ Body_mass_female_mean, data = d), you're examining how female body mass (x) might predict or relate to maximum lifespan (y).
```

## Categorical Predictors

height as a function of sex, eye color, two things (male female or blue, brown etc.)

behind the scenes r is making k discrete groups as "dummy" values for our categorical

### Data set

```{r}
library(dplyr)
f <- "https://raw.githubusercontent.com/difiore/ada-datasets/refs/heads/main/AVONETdataset1.csv"
d <- read_csv(f, col_names = TRUE)
head (d)

d <- d |>
  select("Species1", "Family1", "Order1", "Beak.Width", "Beak.Depth")

# which values are categorical 
   # 

```

# March 27 

## Regression with Categorical Predictors 

### Categorical data 

```{r}



library(dplyr)
library(tidyverse)
library(jtools)
f <- "https://raw.githubusercontent.com/difiore/ada-datasets/refs/heads/main/AVONETdataset1.csv"
d <- read_csv(f, col_names = TRUE)
head (d)

d <- d |>
  select("Species1", "Family1", "Order1", "Beak.Width", "Beak.Depth", "Tarsus.Length", "Wing.Length", "Tail.Lenght", "Mass", "Habitat")

# which values are categorical 
   # 
```

```{r}
ggplot(data = d |> drop_na(Trophic.Level), 
       aes(x = Trophic.Level, y = log(Mass)))+
  geom_boxplot()+
  geom_jitter()
ggplot(data = d |> drop_na(Migration), 
       aes(x = Migration, y = log(Mass)))+
  geom_boxplot()+
  geom_jitter()
```

```{r}
ggplot(data = d |> drop_na(Habitat), aes(x = Habitat, y = log(Mass)))+
  geom_boxplot() + geom_jitter()
```

Run a linear modes for Anova analysis look at results

migration is stored

```{r}
glimpse(d)
# have to make migration into categorrical variables by using as.factor function 

# or mutate migration variable

migration1<- lm(log(Mass) ~ Trophic.Level, data = d)
migration2 <-lm(log(Mass)~ as.factor(Migration), data = d)
summary (migration1)
summary(migration2)

```

```{r}
ggplot(data = d |> drop_na(Trophic.Level), aes(x = Trophic.Level, y = log(Mass))) + geom_boxplot()
```

Carnivore is the baseline level that everything is compared to it is telling you how much higher the mean is for herbivores vs. the mean for carnivores.

We can re-level what the base level is but r automatically sorts what the base level is via alphabetical order

T-value (estimate / standard error)

probability of seeing expectation, chance of seeing that by chance

via plot scavengers are statistically different from body size of carnivores

statistical difference of being an herbivore and being a carnivore

very small p values

```{r}
d <- d |> mutate(Trophic.Level = relevel(as.factor(Trophic.Level), ref = "Herbivore"))

#relevel again

d <- d |> mutate(Trophic.Level = relevel(as.factor(Trophic.Level), ref = "Omnivore"))

migration1<- lm(log(Mass) ~ Trophic.Level, data = d)
migration2 <-lm(log(Mass)~ as.factor(Migration), data = d)
summary (migration1)
summary(migration2)
```

F-statistic Degrees of freedom\
Calculating the p value of the overall model

is body mass associated with trophic level, not focusing on anyone predictor

```{r}
pf(78.42, df1 = 3, df2 = 11000, lower.tail = FALSE)
```

### Then we want to know if there as any categorical affect

### Post-Hoc Tests

after finding a significant omnibus F statistic in an ANOVA , we can test, post-hoc, what group means are different from one another using pairwise t tests with an appropriate p value correction or a turkey honest significant differences test

```{r}
# comparing body mass and trophic level 
# carniovres and 
(pairwise.t.test(log(d$Mass), d$Trophic.Level, p.adj = "bonferroni"))
m1 <- aov(log(Mass) ~ Trophic.Level, data = d)


(posthoc <- TukeyHSD(m1, which = "Trophic.Level", conf.level = .95))

```

# permutation approach to inference in ANOVA 

short cut to pull out f statistic

```{r}


original.F <- aov(log(Mass) ~ Trophic.Level, data = d) |>
  broom::tidy() |>
  filter(term == "Trophic.Level")
original.F #F statistic  and p value for omnibus F test 
```

### Now generate permutation statistic using infer package

```{r}
library(infer)
d <- d |> mutate(logMass = log(Mass))
permuted.F <- d |>
  specify(logMass ~ Trophic.Level)|>
  hypothesise(null = "independence")|>
  generate(reps = 1000, type = "permute")|>
  calculate(stat = "F")

visualize(permuted.F) +
            shade_p_value(obs_stat =  original.F$statistic, direction = "greater")
```

# Multi-factor ANOVA (e.g., 2-way ANOVA)

sometimes we are interested in more than 1 predictor

example Trophic level and migration
